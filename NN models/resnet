'''
https://www.kaggle.com/pourchot/in-python-tabular-denoising-residual-network
The NN has 3 inputs and a residual block :

Quantile inputs
Bins inputs
Denoised + PCA inputs
*For comparison purpose, I try to keep the same files names as much as possible. Some R functions have no exact equivalent functions in Python (as far as I know) so there are some functions definition and workaround.

'''

## NN Model definition

def get_res_model():
    
    # Quantile-normalized input:
    inputQ = layers.Input(shape = (100))

    # Encoded + principal components input:
    inputE = layers.Input(shape= (elabeled.shape[1]))

    # Quantile-binned input:
    inputB = layers.Input(shape = (100))
    
    # Feed-forward block:
    denseQE = layers.Dropout(0.3)(layers.Concatenate()([inputQ,inputE]))
    denseQE = tfa.layers.WeightNormalization(
                layers.Dense(
                units = 300,
                activation='elu',
                kernel_initializer = "lecun_normal"))(denseQE) 
    
   # Embedding + convolutional block:
    embedB = layers.Embedding (input_dim = 51, 
                              output_dim = 10,
                              embeddings_regularizer='l2',
                              embeddings_initializer='lecun_uniform')(inputB)
    embedB = layers.Dropout(0.3)(embedB)
    embedB = layers.Conv1D(10,1,activation = 'relu')(embedB)
    embedB = layers.Flatten()(embedB)
    
    # Residual block:
    hidden = layers.Dropout(0.3)(layers.Concatenate()([denseQE,embedB]))
    hidden = tfa.layers.WeightNormalization(
                layers.Dense(
                units = 32,
                activation='elu',
                kernel_initializer = "lecun_normal"))(hidden) 
    
    output = layers.Dropout(0.3)(layers.Concatenate()([embedB,hidden]))
    output = tfa.layers.WeightNormalization(
                layers.Dense(
                units = 32,
                activation='relu',
                kernel_initializer = "lecun_normal"))(output) 
    
    output = layers.Dropout(0.4)(layers.Concatenate()([embedB,hidden,output]))
    output = tfa.layers.WeightNormalization(
                layers.Dense(
                units = 32,
                activation='selu',
                kernel_initializer = "lecun_normal"))(output) 
    output = layers.Dense(
                units = 1, 
                activation ='selu',
                kernel_initializer ="lecun_normal")(output)
    # Output:
    model = Model([inputQ,inputE,inputB],output)
    model.compile(loss='mse',
                  metrics=[tf.keras.metrics.RootMeanSquaredError()],
                  optimizer = keras.optimizers.Adam(lr=0.005))
    
    return model
    
## Training & prediction

N_FOLDS = 10
SEED = 1
EPOCH = 100
N_round = 3

oof = np.zeros((y.shape[0],1))
pred = np.zeros((test.shape[0],1))

for i in range (N_round):
    
    oof_round = np.zeros((y.shape[0],1))
    skf = StratifiedKFold(n_splits=N_FOLDS, 
                          shuffle=True, 
                          random_state=SEED *i
                         )

    for fold, (tr_idx, ts_idx) in enumerate(skf.split(X,y)):
        print(f"\n------ TRAINING ROUND {i} FOLD {fold} ------\n")
        
        # Normalized:
        qtrain = qlabeled[tr_idx]                       
        qvalid = qlabeled[ts_idx]
        
        # Binned:
        btrain = blabeled[tr_idx]
        bvalid = blabeled[ts_idx]
        
        # Encoded:
        etrain = elabeled[tr_idx]
        evalid = elabeled[ts_idx]
        
        # target:
        ytrain = y[tr_idx]
        yvalid = y[ts_idx]
 
        K.clear_session()

        #================= MODEL training =========

        model= get_res_model()
        model.fit([qtrain, etrain, btrain],
                  ytrain,
                  batch_size = 2048, 
                  epochs = EPOCH,
                  validation_data=([qvalid, evalid, bvalid],
                  yvalid),
                  callbacks=[es, plateau],
                  verbose = 0)

        #============== Model prediction ==========
        
        pred_round = model.predict([qvalid, evalid, bvalid]) 
        oof[ts_idx] += pred_round / N_round
        oof_round[ts_idx] += pred_round
        score_NN_round = math.sqrt(mean_squared_error(yvalid, pred_round))
        print(f"\nFOLD {fold} round {i} Score model: {score_NN_round}\n")
        pred += model.predict([qunlabeled, eunlabeled, bunlabeled]) / (N_FOLDS * N_round)
        
    score_round = math.sqrt(mean_squared_error(y, oof_round))
    print(f"\==== FINAL SCORE round {i} REGRESSION MODEL === : {score_round}===\n")
    
score_round = math.sqrt(mean_squared_error(y, oof))
print(f"\n***** FINAL SCORE REGRESSION MODEL : {score_round}*****\n") 

'''
sample_submission['loss'] = pred
sample_submission.to_csv('submission37.csv',index = False)
display(pd.read_csv("submission37.csv"))
'''
