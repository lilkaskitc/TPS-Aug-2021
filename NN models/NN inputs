'''
https://www.kaggle.com/pourchot/in-python-tabular-denoising-residual-network

The NN has 3 inputs and a residual block :

Quantile inputs
Bins inputs
Denoised + PCA inputs



train = tr.copy()
test = ts.copy()
y = np.array(train['loss'])
X = train.drop(['id','loss'],axis = 1)
test = test.drop(['id'],axis = 1)
xall = pd.concat([X,test],axis=0,copy=False).reset_index(drop=True)
y.shape,X.shape,test.shape,xall.shape
'''

## Quantile Normalization

xmedian = pd.DataFrame.median(xall,0)
x25quan = xall.quantile(0.25,0)
x75quan = xall.quantile(0.75,0)
xall = (xall-xmedian)/(x75quan-x25quan)
def quantile_norm(df_input):
    sorted_df = pd.DataFrame(np.sort(df_input.values,axis=0), index=df_input.index, columns=df_input.columns)
    mean_df = sorted_df.mean(axis=1)
    mean_df.index = np.arange(1, len(mean_df) + 1)
    quantile_df =df_input.rank(axis = 0, method="min").stack().astype(int).map(mean_df).unstack()
    return(quantile_df)

qall = np.array(quantile_norm(xall))
qlabeled = qall[:len(train),:]
qunlabeled = qall[len(train):,:]

## Simple bins creation

# 100 bins for the bins head of the NN :
ball = np.zeros((400000,100))
for i in range(X.shape[1]):
    ball[:,i] = pd.qcut(qall[:,i],50,labels=False,duplicates = 'drop')
blabeled = ball[:X.shape[0],:]
bunlabeled = ball[X.shape[0]:,:]

## Denoiser AutoEncoder

noise = np.random.normal(0, .1, (400000,100))
qall = np.array(qall)
xnoisy = qall + noise

# Split 80% training / 20% validation sets:
xtrain = xnoisy[0:320000,:]
ytrain = qall[0:320000,]
xvalid = xnoisy[320000:400000,:]
yvalid = qall[320000:400000,:]
es = tf.keras.callbacks.EarlyStopping(
    monitor= 'val_loss',
    min_delta=1e-7, 
    patience=20, 
    verbose=0,
    mode='min', 
    baseline=None, 
    restore_best_weights=True)

plateau = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss', 
    factor=0.8, 
    patience=4, 
    verbose=0,
    mode='min')
def custom_loss(y_true, y_pred):
    loss = K.mean(K.square(y_pred - y_true))
    return loss
def autoencoder():
    ae_input = layers.Input(shape = (100))
    ae_encoded = layers.Dense(
        units = 100,
        activation='elu')(ae_input)
    ae_encoded = layers.Dense(
        units = 300,
        activation='elu')(ae_encoded)
    ae_decoded = layers.Dense(
        units = 100,
        activation='elu')(ae_encoded)
    
    return Model(ae_input, ae_decoded),Model(ae_input,ae_encoded)
autoencoder,encoder = autoencoder()
autoencoder.compile(loss= custom_loss,
                    optimizer = keras.optimizers.Adam(lr=5e-3))
history = autoencoder.fit(xtrain,
                    ytrain, 
                    epochs=200,
                    batch_size = 512,
                    verbose = 0,
                    validation_data=(xvalid,yvalid),
                    callbacks=[es,plateau])
eall = encoder.predict(qall) # data encoding by denoiser encoder
print("max encoded value =",np.max(eall)) # check if the denoising was successful for variance

## Features encoded selection according to variance threshold

evar = np.var(eall, axis=0,ddof=1)
evar1 = evar > 0.8  # Threshold variance
a = np.where(evar1 == False,evar1,1)
nb_col = a.sum()
print("number of selected columns",nb_col)
if ((nb_col < 90) | (nb_col > 110)) == True:
    sys.exit()  # Stop in case of a too small or too large number of features selected
eall_1=pd.DataFrame()
for i in range(300):
    if evar1[i] == True:
        colname = f'col_{i}'
        eall_1[colname] = eall[:,i]
eall_1 = np.array(eall_1)
elabeled = eall_1[:len(train),:]
eunlabeled = eall_1[len(train):,:]
elabeled.shape,eunlabeled.shape

## PCA on encoded features

pca = PCA(n_components=10)
pall = pca.fit_transform(eall)
sc_pca = StandardScaler()
pall = sc_pca.fit_transform(pall)

## Merge of PCA + encoded features (after threshold)

plabeled = pall[:len(train),:]
punlabeled = pall[len(train):,:]
elabeled = np.hstack((elabeled,plabeled))
eunlabeled = np.hstack((eunlabeled,punlabeled))

